# Math Agent - Automated Lean Theorem Prover

This project uses Large Language Models (LLMs) to automatically generate proofs for mathematical theorems written in the Lean 3 programming language. It is designed to work with the `miniF2F` benchmark, a collection of formal-to-formal theorems from high-school math competitions.

The core workflow is as follows:
1.  **Parse Theorems**: A Lean file containing multiple theorem statements is parsed to extract each theorem individually.
2.  **Generate Proofs**: Each theorem statement (with its proof replaced by `sorry`) is sent to an LLM, which is prompted to generate a complete proof.
3.  **Verify Proofs**: The proof generated by the LLM is placed back into a temporary Lean file and verified using the Lean 3 compiler to check for correctness.

## Project Structure

-   `miniF2F/`: A Git submodule containing the `miniF2F` dataset of theorems. This provides the mathematical problems for the agent to solve.
-   `process_lean.py`: A Python script to parse a `.lean` file (e.g., from `miniF2F`), extract all theorem statements, and save them into a structured `valid.json` file.
-   `test_sonnet.py`: The main script that reads theorems from `valid.json`, uses an LLM (via OpenRouter) to generate proofs, and verifies them using a local Lean 3 installation.
-   `requirements.txt`: A file listing the required Python dependencies for this project.
-   `log/`: Directory where log files are stored.
-   `tmp/`: Directory for temporary files created during proof verification.

## Setup

### 1. Prerequisites

-   **Python 3.8+**: Make sure you have a modern version of Python installed.
-   **Lean 3**: This project requires a working installation of Lean 3.42.1. Follow the [official installation instructions](https://leanprover-community.github.io/get_started.html). Ensure the `lean` executable is in your system's PATH.

### 2. Clone the Repository

Clone this repository and its submodules:

```bash
git clone --recurse-submodules https://github.com/umbra2728/math_agent_vikhr.git
cd math_agent_vikhr
```

If you have already cloned the repository without the submodules, you can initialize them by running:
```bash
git submodule update --init --recursive
```

### 3. Install Dependencies

Install the required Python packages using pip:

```bash
pip install -r requirements.txt
```

### 4. Set Up API Key

This project uses the OpenRouter API to interact with various LLMs. You need to set your OpenRouter API key as an environment variable.

```bash
export OPENROUTER_API_KEY="your_openrouter_api_key"
```
You can add this line to your shell's startup file (e.g., `.bashrc`, `.zshrc`) to make it permanent.

## Usage

The workflow is a two-step process: first, you process the Lean source files into a JSON format, and then you run the theorem-proving agent.

### Step 1: Process Lean Files

Run `process_lean.py` to parse the theorems from the `miniF2F` submodule and generate the `valid.json` file. This file will contain all the problems for the LLM to solve.

```bash
python process_lean.py
```

This script will read `miniF2F/lean/src/valid.lean`, extract all theorems, and create `valid.json` in the project root.

### Step 2: Run the Prover Agent

Execute the `test_sonnet.py` script to start the automated proving process. The script will select a subset of theorems from `valid.json`, ask the LLM to prove them, and verify the results.

```bash
python test_sonnet.py
```

You can customize the agent's behavior with the following command-line arguments:

-   `--subset_size`: The number of theorems to select for the run. Default is 10.
    ```bash
    python test_sonnet.py --subset_size 20
    ```
-   `--json_file`: Path to the JSON file with theorems. Defaults to `valid.json`.
    ```bash
    python test_sonnet.py --json_file /path/to/your/theorems.json
    ```
-   `--model`: The OpenRouter model name to use. Defaults to `anthropic/claude-sonnet-4`.
    ```bash
    python test_sonnet.py --model "google/gemini-pro"
    ```
-   `--log_level`: Set the logging level (`DEBUG`, `INFO`, `WARNING`, `ERROR`). `DEBUG` is useful for detailed output.
    ```bash
    python test_sonnet.py --log_level DEBUG
    ```

## How It Works

### `process_lean.py`

This script reads a `.lean` file containing multiple declarations (`theorem`, `lemma`, `def`, etc.). It uses regular expressions to identify each declaration block, extract its name and full statement, and check if it already contains a proof. If a proof exists (i.e., the content between `begin` and `end` is not `sorry`), it is replaced with `begin sorry end`. This normalization step ensures all theorems are presented as unsolved problems to the LLM. The final list of processed theorems is saved as a JSON array in `valid.json`.

### `test_sonnet.py`

This is the core agent.
1.  **Initialization**: It parses command-line arguments, sets up logging, and initializes the OpenRouter client.
2.  **Theorem Loading**: It reads the `valid.json` file.
3.  **Subset Selection**: It selects a stratified random sample of theorems to work on, preserving the original ratio of solved-to-unsolved problems. This is useful for getting a representative score without running on the entire dataset.
4.  **Proof Generation Loop**: For each selected theorem, it:
    a.  Constructs a system prompt and a user prompt, instructing the LLM to act as a Lean 3 expert and complete the proof.
    b.  Sends the request to the specified LLM via the OpenRouter API.
    c.  Extracts the Lean proof code from the model's response. It specifically looks for a `begin...end` block or a `by...` tactic.
5.  **Verification**: The extracted proof is inserted into a temporary `.lean` file, which also includes the necessary `import minif2f_import` line. The script then runs `lean --make` from the `miniF2F` directory. This compiles the file and checks it for errors.
    - If the compilation is successful and a `.olean` file is produced, the proof is considered **valid**.
    - If there are any errors or the compilation times out, the proof is considered **invalid**.
6.  **Reporting**: After processing all theorems in the subset, the script prints a summary of how many proofs were successfully generated and verified, along with a pass rate. 